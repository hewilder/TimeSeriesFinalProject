---
title: "Final Project"
author: "Hannah Wilder and Chathura Gunasekara"
date: "April 9, 2016"
output: pdf_document
geometry: margin=.5in
---

Notes:
Possible source of population data: http://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/3105.0.65.0012014?OpenDocument

Change working directory here
```{r setup, include=FALSE}
library(TSA)

#Hannah
#setwd( "C:/Users/Hannah/Downloads/2016SpringTextbooks/TimeSeriesAnalysis/FinalProject/TimeSeriesFinalProject")

#Chathura
#test
#setwd("./")
```


Load data (assumes file is in working directory)
```{r}
#load the data
beerData<-read.csv("monthly-beer-production-in-austr.csv")

#cut off the last row which is NA
beerData<-beerData[-nrow(beerData),]
colnames(beerData)<-c("Month", "Production")

#turn into time series also hold back the last two years of data for forecasting
beerTS<-ts(beerData[1:(nrow(beerData)-24),2], frequency=12, start=c(1956,1))
beer_forecast<-ts(beerData[(nrow(beerData)-23):nrow(beerData), 2], start=c(1993,9), frequency=12)




```

#Plot data
```{r}
library(TSA)
par(mfrow=c(1,1))
plot(beerTS, main="Beer Production in Australia by Month")

plot(beerTS, main="Beer Production in Australia by Month (seasons marked)", type="l")
points(y=beerTS, x=time(beerTS), pch=as.vector(season(beerTS)))


```

#Another plot to show seasonality
```{r warning=FALSE, message=FALSE}
require(fpp)
seasonplot(beerTS,year.labels=TRUE,ylab="megaliters",main="Seasonal plot: quarterly beer production", col=rainbow(20), pch=19)
```

In the plot we see obvious seasonality with higher production in November and December and lower production in June and July. There is a trend which may be difficult to fit as it doesn't appear to be a "well known" function like a linear or quadratic function, so we'll have to experiment. It also looks like the variance of the data is larger in the middle, so we will probably want to take the log of our data to correct that varaince issue. 

#Decompsing the time series to see trends and patterns
```{r}
decompbeer = decompose (beerTS, type="additive")
plot (decompbeer)

#monthplot(beerTS, main="Decomposition of Series by Month")
```


#Investigate possible relationship with population data

```{r, fig.width=4, fig.height=2}
#load population data
library(reshape)
library(ggplot2)

#Clean up population data
pop_totalData<-t(read.csv("Pop_total.csv", row.names=1))
dropCols<-colnames(pop_totalData) %in% c("Unspecified","Period not indicated")
rownames(pop_totalData)<-c(1921:2011)
pop_totalDataLong<-pop_totalData[,!dropCols]
pop_totalData<-pop_totalData[paste(1956:1995),!dropCols]

#Aggregate beer data
beerYear<-seq(from=1956, to=1996, by=1)
beerYear<-rep(beerYear, each=12)
beerYear<-beerYear[1:nrow(beerData)]
beerAg<-aggregate(beerData[,2], FUN=mean, by=list(year=beerYear))

#Attach to beer data
beerPop<-data.frame(cbind(beer=beerAg[,2],pop_totalData))
beerPopScale<-scale(beerPop)

beerPopRes<-melt(beerPopScale, variable.name="series")
colnames(beerPopRes)<-c("time", "series", "stddev")

allNames<-colnames(beerPop)[2:length(colnames(beerPop))]

#Plot data for each age group and beer data on same plot
par(mfrow=c(2,2))
for (name in allNames) {
  subset_data<-subset(beerPopRes, beerPopRes$series%in%c("beer", name))
  newPlot<-ggplot(subset_data, aes(time,stddev)) + geom_line(aes(colour = series)) +ggtitle(paste("Pop (age group ",name,") and Beer Prod"))
  print(newPlot)
}
par(mfrow=c(1,1))

#Make a model based on the 15-19 age group
yearModel1<-lm(beer ~ X15.19, data=beerPop)
```

It appears that there may be a relationship with the 15-19 age block, which makes sense since the legal drinking age is 18. We may be able to use this to remove some of our trend. However, the 10-14 age group numbers look like they might have potential if shifted forward a few years. This makes sense since these children will grow up and start drinking beer. 


```{r}
#Explore lagged x10.14 data
laggedData<-data.frame(beer=beerAg[,2])
models<-list()
modelRsq<-c()
for (lag in 0:8) {
  newColNames<-c(colnames(laggedData), paste("lag", lag, sep=""))
  newLag<-pop_totalDataLong[paste(1956:1995-lag), "10-14"]
  laggedData<-data.frame(laggedData, newLag)
  newModel<-lm(beer ~ newLag, data=laggedData)
  models[[paste("lag", lag, sep="")]]<-newModel
  modelRsq<-c(modelRsq, summary(newModel)$r.squared)
  colnames(laggedData)<-newColNames
  
}

plot(modelRsq, main="R Squared Values for lags of 10-14 age group", xlab="lag number", ylab="R squared value")
abline(v=6, lty=2)

lagDataScale<-scale(laggedData)[,c(1,8)]
lagDataMelt<-melt(lagDataScale, variable.name="series")
colnames(lagDataMelt)<-c("time", "series", "stddev")

newPlot<-ggplot(lagDataMelt, aes(time,stddev)) + geom_line(aes(colour = series)) +ggtitle(paste("Lags of 10-14 Pop and Beer Prod"))
print(newPlot)

plot(ts(residuals(models[["lag6"]]), frequency=1, start=c(1956)), main="Residuals from modeling beer production with 10-14 lag 6", ylab="Residual")
abline(h=0)

lag6_resid<-ts(residuals(models[["lag6"]]), frequency=1, start=c(1956))
adf.test(laggedData$beer)
acf(laggedData$beer)
pacf(laggedData$beer)
ar1YearlyModel<-arima(laggedData$beer, order=c(1,1,0), xreg=laggedData$lag6)

```

We see that lag 6 is the optimal lag in terms of R-squared values. This makes sense because in 6 years, this age group will be 16-20, or right around drinking age. We can see in the residuals that it isn't perfect, but this pattern may be easier to model than what we had before, it looks much more like a regular polynomial. 

We can model just the yearly trend, but this isn't a stationary model, so we want to try to model the monthly data with the seasonality. 
 
If we are interested in modeling the seasonal trend in our data, we would need to extend the population data to a monthly series. To do this, we need to assume that the population will change at about the same rate over the year or that it will remain approximately constant over a year. In this case we will choose to assume that the population will change at about the same rate over the year. This may not be true if there is a particular month in which there is a high influx of immigrants, however it seems reasonable to assume that the majority of the population from year to year is composed of people who were in Australia the previous year (source for net migration rate preferrably broken up by age group needed).  
#Interpolate Monthly Numbers
```{r}
library(zoo)

#Create a vector with missing values for zoo to interpolate
withNA<-c()
for (year in 1:nrow(laggedData)) {
  withNA<-c(withNA, laggedData$lag6[year], rep(NA, 11))
}

#Create a vector with missing values for zoo to interpolate
withNALong<-c()
for (year in 1:nrow(pop_totalDataLong)) {
  withNALong<-c(withNALong, pop_totalDataLong[year, "10-14"], rep(NA, 11))
}


#Interpolate values using zoo library
zooSeries<-zoo(withNA, frequency=12)
wAppx<-na.approx(zooSeries, na.rm=FALSE)
monthlyLag6<-wAppx

#Interpolate long series for forecasting
zooSeriesLong<-zoo(withNALong, frequency=12)
wAppxLong<-data.frame(na.approx(zooSeriesLong, na.rm=FALSE))
rownames(wAppxLong)<-round(seq(from=(1921+6), length.out=nrow(wAppxLong), by=1/12),2)
monthlyLag6Long<-wAppxLong

#Reattach to beer numbers for plotting
beerPopMonth<-data.frame(beer=beerTS, lag6_10_14=monthlyLag6[1:length(beerTS)])
rownames(beerPopMonth)<-round(seq(from=1956, length.out=length(beerTS), by=1/12),2)
scaleMonth<-scale(beerPopMonth)
scaleMonthMelt<-melt(scaleMonth, variable.name="series")
colnames(scaleMonthMelt)<-c("time", "series", "stddev")

#Make a pretty plot
newPlot<-ggplot(scaleMonthMelt, aes(time,stddev)) + geom_line(aes(colour = series)) +ggtitle(paste("Lag 6 of 10-14 Pop and Beer Prod"))
print(newPlot)
```

Now we are ready to investigate models using the population numbers

```{r}
monthlyPopModel<-lm(beer ~ lag6_10_14, data=beerPopMonth)
summary(monthlyPopModel)

residFromPop<-ts(residuals(monthlyPopModel), frequency=12, start=c(1956,1))
plot(residFromPop, type="l", main="Plot of residuals from Pop model")
points(y=residFromPop, x=time(residFromPop), pch=as.vector(season(residFromPop)))
abline(h=0)

```

We notice that there still appears to be pattern in the residuals and that the variance appears to be larger at later lags, so the first thing we should do is log the beer data to fix the variance problem. We can also try to fit the remainder of the pattern with a polynomial

```{r}
logBeer<-log(beerPopMonth$beer)
lag6_10_14<-beerPopMonth$lag6_10_14
t<-1:length(beerTS)
t2<-t^2
t3<-t^3
t4<-t^4
monthlyPopModel_log<-lm(logBeer ~ lag6_10_14+t+t2+t3+t4)
summary(monthlyPopModel_log)

residFromPop_log<-ts(residuals(monthlyPopModel_log), frequency=12, start=c(1956,1))
plot(residFromPop_log, type="l", main="Plot of residuals from Pop/Poly Trend logged model")
points(y=residFromPop_log, x=time(residFromPop_log), pch=as.vector(season(residFromPop_log)))
abline(h=0)
```

Obviously there is a pattern in the residuals here, so we can either try to fit the remainder of the trend or continue on and count that as error in favor of a more simple model. For now we will continue on, but remember to check the residuals of any model we come up with.




```{r}
adf.test(residFromPop_log)
pp.test(residFromPop_log)
acf(residFromPop_log)
pacf(residFromPop_log)
eacf(residFromPop_log)
```

Based on the pacf plot, perhaps p=1, P=1 in an SARIMA(1,0,0)(1,0,0)[12]

Write up a quick function for plotting acf and pacf of residuals
```{r}
getModelString<-function(model) {
  modSpec<-model$arma
  modelString<-paste("SARIMA(", modSpec[1],",", modSpec[6], ",", modSpec[2],")(", modSpec[3], ",", modSpec[7], ",", modSpec[4],")[", modSpec[5],"]", sep="")
  
  return(modelString)
}

plotResid<-function(model) {
  residuals<-ts(residuals(model), frequency=12, start=c(1956,1))
  modelString<-getModelString(model)
  par(mfrow=c(1,1))
  acf(residuals, main=paste("ACF of", modelString), lag.max=40, cex=.5)
  pacf(residuals, main=paste("PACF of", modelString), lag.max=40, cex=.5)
  par(mfrow=c(1,1))
}

```

```{r}
popModel1<-arima(residFromPop_log, order=c(1,0,0), seasonal=list(order=c(1,0,0), period=12))
plotResid(popModel1)
```

There is still a significant autocorrelation at lag 12 in both plots. Perhaps try either p=12 or q=12?

```{r}
popModel2<-arima(residFromPop_log, order=c(12,0,0), seasonal=list(order=c(1,0,0), period=12))
plotResid(popModel2)
tsdiag(popModel2)

popModel3<-arima(residFromPop_log, order=c(1,0,12), seasonal=list(order=c(1,0,0), period=12))
plotResid(popModel3)
tsdiag(popModel3)
```

That definitely looks better, we still have significant lags, but they now lie very close to the boundary. This is true more so for the SARIMA(12,0,0)(1,0,0)[12] model than the SARIMA(0,0,12)(1,0,0)[12] model. 

Try overfitting the SARIMA(12,0,0)(1,0,0)[12]
```{r}
#popModel4<-arima(residFromPop_log, order=c(13,0,0), seasonal=list(order=c(1,0,0), period=12))
#Produces error

popModel5<-arima(residFromPop_log, order=c(12,0,1), seasonal=list(order=c(1,0,0), period=12))
popModel5

popModel6<-arima(residFromPop_log, order=c(12,0,0), seasonal=list(order=c(2,0,0), period=12))
popModel6

popModel7<-arima(residFromPop_log, order=c(12,0,0), seasonal=list(order=c(1,0,1), period=12))
popModel7
```

We see that the extra coefficients added in models 6 (`r getModelString(popModel6)`) and 7 (`r getModelString(popModel7)`) are both significant, so we try adding them to the model together.

```{r}
popModel8<-arima(residFromPop_log, order=c(12,0,0), seasonal=list(order=c(2,0,1), period=12))
popModel8
```

With both together, the sar2 coefficent is no longer significant, overfit model 7 (`r getModelString(popModel7)`)

```{r}
popModel9<-arima(residFromPop_log, order=c(13,0,0), seasonal=list(order=c(1,0,1), period=12))
popModel9

popModel10<-arima(residFromPop_log, order=c(12,0,1), seasonal=list(order=c(1,0,1), period=12))
popModel10

popModel11<-arima(residFromPop_log, order=c(12,0,0), seasonal=list(order=c(1,0,2), period=12))
popModel11
```

The new coefficients are not significant (with the exception of model 9, which appears to have some issues with fitting), suggesting that we should stick with model 7 (or possibly 8).


Overfit model 8 (`r getModelString(popModel8)`)

```{r}
#popModel12<-arima(residFromPop_log, order=c(13,0,0), seasonal=list(order=c(2,0,1), period=12))
#popModel12
#Produces error in optim formula

popModel13<-arima(residFromPop_log, order=c(12,0,1), seasonal=list(order=c(2,0,1), period=12))
popModel13

popModel14<-arima(residFromPop_log, order=c(12,0,0), seasonal=list(order=c(2,0,2), period=12))
popModel14

popModel15<-arima(residFromPop_log, order=c(12,0,0), seasonal=list(order=c(3,0,1), period=12))
popModel15
```

Let auto.arima choose a model

```{r}
autoRes<-auto.arima(residFromPop_log, max.p=13, max.order=14, test="adf")
autoRes
plotResid(autoRes)
```

auto.arima chose `r getModelString(autoRes)` but this model still has the same problems as the others we have tried so far (error terms not independent) and the AIC is larger than models 7 and 8, so the only reason we would want to consider this model is if we were really interested in a more parsimonious model. 

#Set up for forecast of population model
#Make the forecasts
Set up external regressor data frame
```{r}
startDate<-round(start(beer_forecast)[1]+start(beer_forecast)[2]/12,2)
endDate<-round(end(beer_forecast)[1]+end(beer_forecast)[2]/12,2)
numToFor<-length(beer_forecast)

allBeerData<-ts(c(beerTS, beer_forecast), start=c(1956, 1), frequency=12)

#Pull data for population
lag6_10_14new<-subset(monthlyLag6Long, as.numeric(rownames(monthlyLag6Long))>=startDate)
lag6_10_14new<-subset(lag6_10_14new, as.numeric(rownames(lag6_10_14new))<=endDate)

tnew<-1:length(allBeerData)
t2new<-tnew^2
t3new<-tnew^3
t4new<-tnew^4

newRegData<-data.frame(t=tnew, t2=t2new, t3=t3new, t4=t4new)
newRegData<-newRegData[(nrow(newRegData)-numToFor+1):nrow(newRegData),]
newRegData<-data.frame(lag6_10_14=lag6_10_14new, newRegData)
colnames(newRegData)<-c("lag6_10_14", colnames(newRegData)[2:5])

predFromPop<-predict(monthlyPopModel_log, newdata=newRegData)

```


#Plot the model forecasts

```{r}
library(TSA)
chosenMod<-popModel8

predictions<-predict(chosenMod, n.ahead=24)
pred<-predictions$pred+predFromPop
uci<-pred+2*predictions$se
lci<-pred-2*predictions$se

ymin=min(c(as.vector(lci),logBeer))-1
ymax=max(c(as.vector(uci),logBeer))+1
plot(logBeer,ylim=c(ymin,ymax),main=modelString, ylab='Logged Beer Production (Megalitres)')
lines(pred,col=2)
lines(uci,col=3)
lines(lci,col=3)

ymin=min(c(as.vector(lci),logBeer))-1
ymax=max(c(as.vector(uci),logBeer))+1
plot(logBeer,xlim=c(1993, 1996), ylim=c(4.5,5.5),main=modelString, ylab='Logged Beer Production (Megalitres)')
lines(pred,col=2)
lines(uci,col=3)
lines(lci,col=3)
lines(log(beer_forecast), col="black")


pred_raw<-(predictions$pred+predFromPop)
pred<-exp(predictions$pred+predFromPop)
uci<-exp(pred_raw+2*predictions$se)
lci<-exp(pred_raw-2*predictions$se)

ymin=min(c(as.vector(lci),beerTS))-1
ymax=max(c(as.vector(uci),beerTS))+1
plot(beerTS,ylim=c(ymin,ymax),main=modelString, ylab='Beer Production (Megalitres)')
lines(pred,col=2)
lines(uci,col=3)
lines(lci,col=3)
lines(beer_forecast, col="black")

ymin=min(c(as.vector(lci),beerTS))-1
ymax=max(c(as.vector(uci),beerTS))+1
plot(beerTS,xlim=c(1993, 1996), ylim=c(ymin,ymax),main=modelString, ylab='Beer Production (Megalitres)')
lines(pred,col=2)
lines(uci,col=3)
lines(lci,col=3)
lines(beer_forecast, col="black")

```






#Try to figure out deterministic trend
```{r}
t<-1:length(beerTS)
t2<-t^2
t3<-t^3
t4<-t^4
t5<-t^5

quadFit<-lm(beerTS~t+t2)
summary(quadFit)

#### plot the data and the fitted quadratic trend function
plot(x=1:length(beerTS),y=beerTS,type='o',ylab="",xlab="Time - Number of Months Since Jan 1956",main="Quadratic Fit on Beer Production Data")
curve(expr = coef(quadFit)[1]+coef(quadFit)[2]*x+coef(quadFit)[3]*x^2+coef(quadFit)[4]*x^3,lty=1,add = TRUE, col="red")

quadFit_resid<-ts(residuals(quadFit),frequency=12, start=c(1956,1)) 
plot(quadFit_resid, main="Residuals from a Quadratic Trend Fit")
abline(h=0)

cubicFit<-lm(beerTS~t+t2+t3)
summary(cubicFit)

#### plot the data and the fitted quadratic trend function
plot(x=1:length(beerTS),y=beerTS,type='o',ylab="",xlab="Time - Number of Months Since Jan 1956",main="Cubic Fit on Beer Production Data")
curve(expr = coef(cubicFit)[1]+coef(cubicFit)[2]*x+coef(cubicFit)[3]*x^2+coef(cubicFit)[4]*x^3,lty=1,add = TRUE, col="red")

cubicFit_resid<-ts(residuals(cubicFit),frequency=12, start=c(1956,1)) 
plot(cubicFit_resid, main="Residuals from a Cubic Trend Fit")
abline(h=0)

order4polyFit<-lm(beerTS~t+t2+t3+t4)
summary(order4polyFit)

#### plot the data and the fitted 4th order polynomial trend function
plot(x=1:length(beerTS),y=beerTS,type='o',ylab="",xlab="Time - Number of Months Since Jan 1956",main="order4poly Fit on Beer Production Data")
curve(expr = coef(order4polyFit)[1]+coef(order4polyFit)[2]*x+coef(order4polyFit)[3]*x^2+coef(order4polyFit)[4]*x^3+coef(order4polyFit)[5]*x^4,lty=1,add = TRUE, col="red")

order4polyFit_resid<-ts(residuals(order4polyFit),frequency=12, start=c(1956,1)) 
plot(order4polyFit_resid, main="Residuals from a order4poly Trend Fit")
abline(h=0)

order5polyFit<-lm(beerTS~t+t2+t3+t4+t5)
summary(order5polyFit)

#### plot the data and the fitted 5th order polynomial trend function
plot(x=1:length(beerTS),y=beerTS,type='o',ylab="",xlab="Time - Number of Months Since Jan 1956",main="order5poly Fit on Beer Production Data")
curve(expr = coef(order5polyFit)[1]+coef(order5polyFit)[2]*x+coef(order5polyFit)[3]*x^2+coef(order5polyFit)[4]*x^3+coef(order5polyFit)[5]*x^4+coef(order5polyFit)[6]*x^5,lty=1,add = TRUE, col="red")

order5polyFit_resid<-ts(residuals(order5polyFit),frequency=12, start=c(1956,1)) 
plot(order5polyFit_resid, main="Residuals from a order5poly Trend Fit")
abline(h=0)

```

It looks like a 4th order polynomial might take care of the worst of it, the question is are we okay with using a 4th order polynomial or should we drop it down to a cubic function and just deal with it? I found population data and I would be interested to see if we can find a good correlation there (total population won't work, I already looked at that, but maybe a specific age group?)

Assume we go with the 4th order polynomial for now. Let's see what we can do about the seasonality with a seasonal means model



```{r}
library(TSA)
month=season(order4polyFit_resid)
seasMeansModel<-lm(order4polyFit_resid~month)
summary(seasMeansModel)
seasMeansModel_resid<-ts(residuals(seasMeansModel),frequency=12, start=c(1956,1)) 
plot(seasMeansModel_resid, main="Residuals from Seasonal Means Model \n(after fitting 4th order polynomial)")
abline(h=0)
```

With an adjusted R-squared value of 71%, this is looking pretty good, but in the residual plot you can still the the variance increasing over time. In addition, there is a noticeable "wave" in the residuals that starts around 1970, but I'm not sure what to do about that yet. For now, let's go back, log the data, and apply both the 4th order polynomial and the seasonal means model at the same time. 

```{r}
logBeer<-log(beerTS)
t<-1:length(logBeer)
t2<-t^2
t3<-t^3
t4<-t^4
month<-season(logBeer)

logSeasPoly<-lm(logBeer~t+t2+t3+t4+month)
summary(logSeasPoly)

logSeasPoly_resid<-ts(residuals(logSeasPoly),frequency=12, start=c(1956,1)) 
plot(logSeasPoly_resid, main="Residuals from Logged Beer\nseasonal Means and 4th order poly fit at same time", type="l")
points(y=logSeasPoly_resid, x=time(logSeasPoly_resid), pch=as.vector(season(logSeasPoly_resid)))
abline(h=0)



```

```{r}
# p & q


adf.test(logSeasPoly_resid)
pp.test(logSeasPoly_resid)
par(mfrow=c(1,2))
acf(logSeasPoly_resid, lag.max=100)
pacf(logSeasPoly_resid, lag.max=100)
par(mfrow=c(1,1))
eacf(logSeasPoly_resid)

qqnorm(logSeasPoly_resid)
qqline(logSeasPoly_resid)
```

```{r}
har.=harmonic(logBeer,5)
modelHR=lm(logBeer~har.+t+t2+t3+t4)
summary(modelHR)
plot(ts(fitted(modelHR),freq=12,start=c(1956,1)),ylab='log Beer',type='l',ylim=range(c(fitted(modelHR),logBeer))); 
points(logBeer)
residFromHR<-ts(residuals(modelHR), frequency=12, start=c(1956,1))
plot(residFromHR)

chosenMod<-modelHR
library(TSA)

har.=harmonic(log(beer_forecast),5)
newRegData<-data.frame(t=tnew, t2=t2new, t3=t3new, t4=t4new)
newRegData<-newRegData[(nrow(newRegData)-numToFor+1):nrow(newRegData),]
newRegData<-data.frame(har., newRegData)
colnames(newRegData)<-c(colnames(har.),"t","t2","t3","t4")

predictions<-predict(chosenMod,newdata =newRegData,se.fit = T)
pred<-ts(predictions$fit,start = c(1993,9),frequency = 12)
uci<-ts(pred+2*predictions$se.fit,start = c(1993,9),frequency = 12)
lci<-ts(pred-2*predictions$se.fit,start = c(1993,9),frequency = 12)
ymin=min(c(as.vector(lci),logBeer))-1
ymax=max(c(as.vector(uci),logBeer))+1
plot(logBeer,ylim=c(ymin,ymax),main=modelString, ylab='Logged Beer Production (Megalitres)')
lines(pred,col=2)
lines(uci,col=3)
lines(lci,col=3)

ymin=min(c(as.vector(lci),logBeer))-1
ymax=max(c(as.vector(uci),logBeer))+1
plot(logBeer,xlim=c(1993, 1996), ylim=c(4.5,5.5),main=modelString, ylab='Logged Beer Production (Megalitres)')
lines(pred,col=2)
lines(uci,col=3)
lines(lci,col=3)
lines(log(beer_forecast), col="black")

```

Let's take a look and see if we have a stationary series yet
```{r}
adf.test(residFromHR)
pp.test(residFromHR)
acf(residFromHR)
pacf(residFromHR)
qqnorm(residFromHR)
qqline(residFromHR)
```






Try an AR(12) model and examine residuals
```{r fig.width=8, fig.height=8}
#Set up external regressors and dummy vars
library(forecast)
monthDummies<-seasonaldummy(logBeer)
externReg<-data.frame(t, t2, t3, t4, monthDummies)

ar12_poly<-arima(logBeer, order=c(12,0,0), xreg=externReg)
ar12_poly
```

We seem to be having trouble getting fits for the trend line, ask about this Monday, try just using the month dummies.

```{r}
ar12<-arima(logBeer, order=c(12,0,0), xreg=monthDummies)
ar12
tsdiag(ar12, gof.lag=20)

#residuals
ar12_resid<-ts(residuals(ar12), frequency=12, start=c(1956,1)) 
plot(ar12_resid, main="AR 12 model Residuals from Logged Beer\nseasonal Means and 4th order poly fit at same time", type="l")
points(y=ar12_resid, x=time(ar12_resid), pch=as.vector(season(ar12_resid)))
abline(h=0)

par(mfrow=c(2,1))
acf(ar12_resid)
pacf(ar12_resid)
shapiro.test(ar12_resid)
LB.test(ar12, lag=35)


pacf_acf<-data.frame(acfVal=acf(ar12_resid, plot=FALSE)$acf, pacfVal=pacf(ar12_resid, plot=FALSE)$acf)
#print(pacf_acf)

chosenMod<-ar12
```


#After we choose a model, run all of the diagnostic tests

```{r fig.width=7, fig.height=4}
chosenMod<-popModel8
modelString<-getModelString(chosenMod)

par(mfrow=c(1,1))
plot(residuals(chosenMod), main=paste("Residuals of Model", modelString), ylab="Residual")
abline(h=0)
```

_Comment:_ 

```{r fig.width=7, fig.height=4}
par(mfrow=c(1,1))
qqnorm(residuals(chosenMod), main=paste("Normal QQ Plot of Residuals from", modelString))
qqline(residuals(chosenMod))

```

_Comment:_ 

```{r fig.width=7, fig.height=4}
par(mfrow=c(1,1))
acf(residuals(chosenMod), main=paste("ACF of Residuals from", modelString))
```

_Comment:_ 


```{r fig.width=7, fig.height=4}
shapiro.test(residuals(chosenMod))

```

_Comment:_ 

```{r fig.width=7, fig.height=4}
LB.test(chosenMod, lag=35)
runs(residuals(chosenMod))
```

_Comment:_ 

#Make the forecasts
Set up external regressor data frame
```{r}
newMonthDummy<-seasonaldummy(beer_forecast)
```


#Plot the model forecasts

```{r}
library(TSA)

TSA::plot.Arima(chosenMod,n.ahead=24,n1=c(1956,1), type='b',ylab='Logged Beer Production (Megalitres)',xlab='Year', col="red", lty=2, cex=.75,newxreg=newMonthDummy)

predictions<-predict(chosenMod, n.ahead=24,newxreg=newMonthDummy)
pred<-predictions$pred
uci<-pred+2*predictions$se
lci<-pred-2*predictions$se

ymin=min(c(as.vector(lci),logBeer))-1
ymax=max(c(as.vector(uci),logBeer))+1
plot(logBeer,ylim=c(ymin,ymax),main=modelString, ylab='Logged Beer Production (Megalitres)')
lines(pred,col=2)
lines(uci,col=3)
lines(lci,col=3)

ymin=min(c(as.vector(lci),logBeer))-1
ymax=max(c(as.vector(uci),logBeer))+1
plot(logBeer,xlim=c(1993, 1996), ylim=c(4.5,5.5),main=modelString, ylab='Logged Beer Production (Megalitres)')
lines(pred,col=2)
lines(uci,col=3)
lines(lci,col=3)
lines(log(beer_forecast), col="black")

```

